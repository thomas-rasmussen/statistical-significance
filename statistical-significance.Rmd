---
title: "Statistical significance"
output:
  html_document:
    toc: true
    toc_depth: 2
    toc_float: true
    collapsed: false
    number sections: true
    self_contained: true
    theme: lumen
bibliography: ./slides/bibliography.bib
csl: ieee.csl
nocite: '@*'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
```

Notes on statistical significance.

# What is a test statistic?

A test statistic is a numerical summary of some sample, $X$, to one value
that can be used to perform a hypothesis test. The test statistic is
chosen/defined in such a way that its sampling distribution under the null
hypothesis of interest is calculable, either exactly or approximately, so it can
be used to calculate _p_-values.

Normal distributed data, unknown variance

$$X \sim N(\mu, \sigma^2),\ \ \ T = \frac{\hat{X} - \mu_0}{s / \sqrt{n}} \sim t(n-1)$$
Here the null hypothesis is usally chosen as $H_0: \mu_0 = 0$. This test
statistic follows a Student's $t$ distribution, a distribution that is
calculable, ie we can easily calculate/estimate the CDF (needed for the p-value).


# What is a _p_-value?

Definition from wikipedia:

Let $t$ be an observed test statistic from an unknown distribution $T$. Then the
_p_-value $p$ is the prior probability of observing a test-statistic value at as
"extreme" as $t$ under the assumed null hypothesis $H_0$. 

Using our normal distribution example, we have that $T$ is symmetric around zero,
so 

$$p = P(|T| \geq |t| | H_0) = 2*F_{t(f)}(-|Z|)$$

The _p_-value is a function of the chosen test statistic, and is therefore a
random variable. If the null hypothesis is true, and the underlying random variable
is continuous then $p \sim U(0, 1)$ (Follow-up on this, should be easy to prove/understand why)

# Example

```{r}

# Simulate data from standard normal distribution
set.seed(1)
n_obs <- 1000
n_dat <- 1000

dat <- data.frame(
  x = rnorm(n_obs * n_dat),
  dataset = rep(1:n_dat, times = n_obs)
  ) %>%
  group_by(dataset) %>%
  summarize(z = (mean(x) - 0)/(sd(x)/sqrt(n_obs))) %>%
  mutate(p = 2*pt(-abs(z), df = n_obs - 1))

# z statistic should have standard normal distribution.
dat %>%
  ggplot(aes(x = z)) +
  geom_density() +
  geom_histogram(aes(y = ..density..), binwidth = 0.2, alpha = 0.5) +
  labs(title = "Test statistic distribtion")

# p-value should be uniformly distributed in [0;1]
dat %>%
  ggplot(aes(x = p)) +
  geom_density() +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, alpha = 0.5) +
  labs(title = "p-value distribution")

# p-value is prior probability of observing new test statistic at least as
# extreme as the observed one.

dat$p[1]

```

The t-statistic in the first sample is z = `r dat$z[1]`. The proportion of
other t-statistics in the samples that are at least as extreme is
`r mean(abs(dat$z[1]) < abs(dat$z[-1]))`, and the p-value for the first sample
is `r dat$p[1]`. These values are close to each other and would converge with
larger amount of data.


# Interpretation of the _p_-value

# Problems with the _p_-value

Besides being hard to interpret correctly, the _p_-values is also sensitive to
the sample size. Everything is "significant" if $n$ is large. 

Take for example, a standard observational study with $n = 10000$, where we make
a descriptive analysis, showing the proportion of a patient characteristic in
each exposure group. Let's say we want to include p-values testing whether or not
the distribution of a dichotomous covariate is equal in each exposure group (not a good
idea in the first place, separate discussion). 

```{r}
set.seed(1)
n <- 10000
n_0 <- n/2
n_1 <- n/2
cov_0 <- rbinom(n_0, 1, 0.51)
cov_1 <- rbinom(n_1, 1, 0.49)
p_1 <- mean(cov_1)
p_0 <- mean(cov_0)
p <- (sum(cov_1) + sum(cov_0)) / (n_1 + n_0)
z <- (p_1 - p_0) / (sqrt(p*(1-p)*(1/n_1 + 1/n_0)))
p_val <- 2*pnorm(-abs(z))
p_val
```
We can see that even though the underlying distribution is basically identical
the test concludes that the distribution is "significantly" different. This might
be true in a mathematical sense, but in this context this is probably not the
appropriate conclusion. 

If $n$ is large, everything is significantly different. Not helpful.

# Confidence intervals

For a given estimate, the 95% confidence interval is the set of all parameter
values for which $p \geq 0.05$.

A common misinterpretation of the "coverage probability" of a  CI is that is
that given a CI then there is a $1-\alpha$ percent probility that the CI contains
the true parameter. This is backwards. The coverage probability of a CI is that
GIVEN the true paramter, an as yet unknown CI has a $1-\alpha$ probability of
including the true value (given correct model assumptions). This is a variation
of the incorrect interpretation of a p-value as a probability of there being a
null-effect.

# Bayesian probability probability intervals

Could be interesting to include something on this. 

# _D_-values

A proposed alternative to _p_-values. Might be out of scope?

# Test code in mathjax

```{r}
val <- 1.4
val_list <- list(val = 1.4)
```

math formula using dynamic value: $val = `r val`$, $val = `r val_list$val`$

$$val = `r val`$$
$$val = `r val_list$val`$$



Renders correctly, only preview in RStudio is incorrect.

# References