---
title: "Statistical significance"
format:
  html:
    self-contained: true
    number-sections: true
    theme: default
    toc: true
    toc-depth: 3
    toc-location: left
    toc-float: true
    toc-title: Contents
    citations-hover: true
    footnotes-hover: true
    code-fold: true
    code-copy: true
    code-tools: true
bibliography: ./slides/bibliography.bib
csl: ama.csl
nocite: '@*'
---

<!-- Note: citations-hover does not work right now for some reason -->

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(dplyr)
library(ggplot2)
library(gtsummary)
```

# What is a test statistic?

A test statistic is a numerical summary of some sample, $X$, to one value
that can be used to perform a hypothesis test. The test statistic is
chosen/defined in such a way that its sampling distribution under the null
hypothesis of interest is calculable, either exactly or approximately, so it can
be used to calculate _p_-values.

Normal distributed data, unknown variance

$$X \sim N(\mu, \sigma^2),\ \ \ T = \frac{\hat{X} - \mu_0}{s / \sqrt{n}} \sim t(n-1)$$
Here the null hypothesis is usally chosen as $H_0: \mu_0 = 0$. This test
statistic follows a Student's $t$ distribution, a distribution that is
calculable, ie we can easily calculate/estimate the CDF (needed for the p-value).


# What is a _p_-value?

Let $t$ be an observed test statistic from an unknown distribution $T$. Then the
_p_-value $p$ is the prior probability of observing a test-statistic value at as
"extreme" as $t$ under the assumed null hypothesis $H_0$. 

The _p_-value can also be viewed as the probability of obtaining an estimate
at least as far from a specified values (eg the null value) as the estimate
we have obtained, if the specified value were the true value. In other words,
the _p_-value is a tail area probability based on the observed effect estimate;
it is calculated as the probability of an effect estimate as large as or larger
than the observed estimtae, assuming the null hypothesis is true.

Note that the _p_-value is a mixed measure: it mixes the estimated effect size
with its estimated precision, both crucial aspects of the data. It is obviously
not possible to present two quantities by one single number. For example, consider
the test statistic $Z$: we can make the test statistic (and therefore the _p_-value)
greater by either increasing the parameter estimate or lowering the estimated standard error. As a consequence, everyting is significant if you just have enough data
(which would lower the standard error).


Using our normal distribution example, we have that $T$ is symmetric around zero,
so 

$$p = P(|T| \geq |t| | H_0) = 2*F_{t(f)}(-|Z|)$$

The _p_-value is a function of the chosen test statistic, and is therefore a
random variable. If the null hypothesis is true, and the underlying random variable
is continuous then $p \sim U(0, 1)$ (Follow-up on this, should be easy to prove/understand why)

# Example

```{r}

# Simulate data from standard normal distribution
set.seed(1)
n_obs <- 1000
n_dat <- 1000

dat <- data.frame(
  x = rnorm(n_obs * n_dat),
  dataset = rep(1:n_dat, times = n_obs)
  ) %>%
  group_by(dataset) %>%
  summarize(z = (mean(x) - 0)/(sd(x)/sqrt(n_obs))) %>%
  mutate(p = 2*pt(-abs(z), df = n_obs - 1))

# z statistic should have standard normal distribution.
dat %>%
  ggplot(aes(x = z)) +
  geom_density() +
  geom_histogram(aes(y = ..density..), binwidth = 0.2, alpha = 0.5) +
  labs(title = "Test statistic distribtion")

# p-value should be uniformly distributed in [0;1]
dat %>%
  ggplot(aes(x = p)) +
  geom_density() +
  geom_histogram(aes(y = ..density..), binwidth = 0.05, alpha = 0.5) +
  labs(title = "p-value distribution")

# p-value is prior probability of observing new test statistic at least as
# extreme as the observed one.

dat$p[1]

```

The t-statistic in the first sample is z = `r dat$z[1]`. The proportion of
other t-statistics in the samples that are at least as extreme is
`r mean(abs(dat$z[1]) < abs(dat$z[-1]))`, and the p-value for the first sample
is `r dat$p[1]`. These values are close to each other and would converge with
larger amount of data.


# Interpretation of the _p_-value

# Problems with the _p_-value

Besides being hard to interpret correctly, the _p_-values is also sensitive to
the sample size. Everything is "significant" if $n$ is large. 

Take for example, a standard observational study with $n = 10000$, where we make
a descriptive analysis, showing the proportion of a patient characteristic in
each exposure group. Let's say we want to include p-values testing whether or not
the distribution of a dichotomous covariate is equal in each exposure group (not a good
idea in the first place, separate discussion). 

```{r}
set.seed(1)
n <- 10000
n_0 <- n/2
n_1 <- n/2
cov_0 <- rbinom(n_0, 1, 0.51)
cov_1 <- rbinom(n_1, 1, 0.49)
p_1 <- mean(cov_1)
p_0 <- mean(cov_0)
p <- (sum(cov_1) + sum(cov_0)) / (n_1 + n_0)
z <- (p_1 - p_0) / (sqrt(p*(1-p)*(1/n_1 + 1/n_0)))
p_val <- 2*pnorm(-abs(z))
p_val
```
We can see that even though the underlying distribution is basically identical
the test concludes that the distribution is "significantly" different. This might
be true in a mathematical sense, but in this context this is probably not the
appropriate conclusion. 

If $n$ is large, everything is significantly different. Not helpful.

# Confidence intervals

For a given estimate, the 95% confidence interval is the set of all parameter
values for which $p \geq 0.05$.

A common misinterpretation of the "coverage probability" of a  CI is that is
that given a CI then there is a $1-\alpha$ percent probility that the CI contains
the true parameter. This is backwards. The coverage probability of a CI is that
GIVEN the true paramter, an as yet unknown CI has a $1-\alpha$ probability of
including the true value (given correct model assumptions). This is a variation
of the incorrect interpretation of a p-value as a probability of there being a
null-effect.

# _p_-values in table 1's

It is not uncommon to encounter so-called table 1's presenting the distribution
of patient characteristics in each exposure group, where _p_-values have been
included to test the comparability of the distribution in each group.

In the case of a randomized controlled trial, this is completely senseless
[@stang_ongoing_2010]. By design, we know that the means of the distributions are
equal, and any departure from this is caused by chance. Testing whether any
difference is caused by chance, is unreasonable, since the answer is always yes,
regardless of the _p_-value. By design (using $\alpha = 0.05$), we would also
expect significant _p_-values once for each 20 covariates on average.

The logic is the same in a propensity score matched or weighted cohort in an
observational study.

For a traditional table 1 in an observational study, including _p_-values, also
seem meaningless. The purpose of the analysis is descriptive: how does the
population look like. There are (typically) no hypotheses about the distribution of
covariates in the population of interest, and whether or not the distributions are "significantly" different has no relevance or impact on the study. It is the magnitude of imbalances between the group that is of interest, and the evaluation of this requires subject matter judgement, that can not be answered by statistical tests.
A mean difference of 0.1 year of age in two treatment groups will be highly
statistical significance in a significantly large population, but it is unlikely
to be clinically relevant, and any regression/causal models in the study would
likely include age, regardless of group distributions, _p_-values or whatever.

In the case of an observational study with the aim of simply describing a
population, _p_-values still seem inappropriate. Large population: everything
is significantly different. Evaluation still requires subject knowledge matter?

In any case, the _p_-value is a poor choice of statistic to evaluate whether or
not there are distribution imbalances? In this case using standardized differences
would be more appropriate?

```{r}
set.seed(1)
n <- 10**4
dat <- tibble(
  group = rbinom(n, 1, 0.5),
  age = rnorm(n, mean = 60 + group, sd = 10),
  biomarker = rnorm(n, mean = 1 + group, sd = 0.5)
)

tbl <- dat %>%
  tbl_summary(
    by = group,
    statistic = list(all_continuous() ~ "{mean} ({sd})"),
    digits = list(age ~ c(1, 1))
  ) %>%
  add_p()

tbl

```

As the example shows it does not take a big population to get small _p_-values.
But is a mean difference of 1 year of age has any clinical revalence? Probably not.
On the other hand, a difference of 1 unit of a biomarker might be enourmous and
very important. Interpretation takes subject matter knowledge.


# Compare parametric and non-parametric p-value

Assume two independent samples with unknown variances. We make no assumption
of normal distributed populations, but we ensure that $n_1 + n_2 > 40$.

To test the hypothesis of equal means, we could either use Welch's t-test or
the Wilcox rank-sum test.

```{r}

set.seed(1)
n <- 10**3
n1 <- n/2
n2 <- n/2
x1 <- rnorm(n1)
x2 <- rnorm(n2, mean = 0.1)

d_0 <- 0
s12 = var(x1)
s22 = var(x2)

t = ((mean(x1) - mean(x2)) - d_0)/(sqrt(s12/n1 + s22/n2))
df = ((s12/n1 + s22/n2)**2) / ((s12/n1)**2/(n1 - 1) + (s22/n2)**2/(n2 - 1))


welch_pval <- 2*pt(-abs(t), df = df)
welch_pval
wilcox_pval <- wilcox.test(x1, x2)$p.value
wilcox_pval

```
Gives approximately the same answers, but the tests are not the same, so 
comparisons should be done with that in mind! 

The Wilcoxon rank-sum test tests the hypothesis that the probability that a
randomly drawn observation from one group is larger than a randomly drawn
observation from the other is equal to 0.5 against an alternative hypothesis
that the probability is not 0.5. In contrast, a t-test tests a null hypothesis
of equal means in two groups against an alternative of unequal means. This is
not the same.

```{r}
set.seed(1)
n <- 10**4
n1 <- n/2
n2 <- n/2
x1 <- rgamma(n1, shape = 5)
x2 <- rnorm(n2, mean = 5)

mean(x1)
mean(x2)

ggplot() +
  geom_density(aes(x = x1)) +
  geom_density(aes(x = x2))

d_0 <- 0
s12 = var(x1)
s22 = var(x2)

t = ((mean(x1) - mean(x2)) - d_0)/(sqrt(s12/n1 + s22/n2))
df = ((s12/n1 + s22/n2)**2) / ((s12/n1)**2/(n1 - 1) + (s22/n2)**2/(n2 - 1))


welch_pval <- 2*pt(-abs(t), df = df)
welch_pval
wilcox_pval <- wilcox.test(x1, x2)$p.value
wilcox_pval
```


# Bayesian probability probability intervals

Could be interesting to include something on this. 

# _D_-values

A proposed alternative to _p_-values. Might be out of scope?

# Test code in mathjax

```{r}
val <- 1.4
val_list <- list(val = 1.4)
```

math formula using dynamic value: $val = `r val`$, $val = `r val_list$val`$

$$val = `r val`$$
$$val = `r val_list$val`$$



Renders correctly, only preview in RStudio is incorrect.

# References
